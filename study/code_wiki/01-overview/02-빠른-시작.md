# 빠른 시작

이 가이드에서는 LeRobot의 주요 기능을 빠르게 체험해봅니다.

---

## 1. 데이터셋 로드 및 탐색

### 기본 데이터셋 로드

```python
from lerobot.datasets import LeRobotDataset

# HuggingFace Hub에서 PushT 데이터셋 로드
dataset = LeRobotDataset("lerobot/pusht")

print(f"총 프레임 수: {len(dataset)}")
print(f"총 에피소드 수: {dataset.num_episodes}")
print(f"에피소드당 평균 프레임: {len(dataset) / dataset.num_episodes:.0f}")
```

### 데이터 구조 확인

```python
# 첫 번째 프레임 가져오기
frame = dataset[0]

print("데이터 키:", frame.keys())
# dict_keys(['observation.image', 'observation.state', 'action', 'episode_index', ...])

# 관측 데이터 확인
print("이미지 shape:", frame["observation.image"].shape)  # (C, H, W)
print("상태 shape:", frame["observation.state"].shape)    # (state_dim,)
print("액션 shape:", frame["action"].shape)               # (action_dim,)
```

### 에피소드 단위 접근

```python
# 첫 번째 에피소드의 모든 프레임
first_episode_indices = dataset.episode_data_index["from"][0:1]
first_episode_frames = dataset[first_episode_indices[0]:first_episode_indices[0]+10]

print(f"에피소드 0의 처음 10 프레임 로드 완료")
```

---

## 2. 사전 훈련된 모델 사용

### ACT Policy 로드

```python
from lerobot.policies.act import ACTPolicy

# HuggingFace Hub에서 사전 훈련된 ACT 모델 로드
policy = ACTPolicy.from_pretrained("lerobot/act_aloha_insertion")

print(f"정책 로드 완료!")
print(f"입력 형태: {policy.config.input_shapes}")
print(f"출력 차원: {policy.config.output_shapes}")
```

### Diffusion Policy 로드

```python
from lerobot.policies.diffusion import DiffusionPolicy

# PushT용 Diffusion 정책
policy = DiffusionPolicy.from_pretrained("lerobot/diffusion_pusht")

print("Diffusion 정책 로드 완료!")
```

---

## 3. 시뮬레이션 환경에서 평가

### 환경 생성

```python
from lerobot.envs.factory import make_env

# PushT 환경 생성
env = make_env("PushT-v0")

# 환경 초기화
observation, info = env.reset(seed=42)

print("환경 초기화 완료!")
print("관측 키:", observation.keys())
```

### 정책 실행

```python
from lerobot.policies.diffusion import DiffusionPolicy

# 정책 로드
policy = DiffusionPolicy.from_pretrained("lerobot/diffusion_pusht")
policy.eval()

# 환경 초기화
observation, info = env.reset(seed=42)

# 에피소드 실행
done = False
step = 0
while not done and step < 300:
    # 정책으로 액션 생성
    with torch.no_grad():
        action = policy.select_action(observation)

    # 환경에서 스텝 실행
    observation, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated
    step += 1

print(f"에피소드 완료! 총 스텝: {step}")
```

---

## 4. 데이터셋 시각화

### CLI로 시각화

```bash
# PushT 데이터셋의 첫 번째 에피소드 시각화
python -m lerobot.scripts.lerobot_dataset_viz \
  --repo-id lerobot/pusht \
  --episode-index 0
```

브라우저에서 [http://localhost:9876](http://localhost:9876)로 접속하면 Rerun 뷰어가 열립니다.

### Python으로 시각화

```python
from lerobot.datasets import LeRobotDataset
import matplotlib.pyplot as plt

# 데이터셋 로드
dataset = LeRobotDataset("lerobot/pusht")

# 첫 10개 프레임의 이미지 시각화
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
for i, ax in enumerate(axes.flat):
    frame = dataset[i]
    # 이미지를 (H, W, C) 형태로 변환
    img = frame["observation.image"].permute(1, 2, 0).numpy()
    ax.imshow(img)
    ax.set_title(f"Frame {i}")
    ax.axis('off')

plt.tight_layout()
plt.show()
```

---

## 5. 간단한 모델 훈련

### 최소 훈련 예제

```python
from lerobot.scripts.lerobot_train import train
from lerobot.configs.train import TrainConfig

# 훈련 설정
config = TrainConfig(
    policy_name="diffusion",
    dataset_repo_id="lerobot/pusht",
    output_dir="outputs/my_first_training",
    num_train_steps=1000,  # 빠른 테스트용
    eval_freq=500,
    save_freq=1000,
    batch_size=32,
    wandb_enable=False,  # WandB 비활성화
)

# 훈련 시작
train(config)
```

### CLI로 훈련

```bash
# 기본 훈련
python -m lerobot.scripts.lerobot_train \
  --policy-name diffusion \
  --dataset-repo-id lerobot/pusht \
  --output-dir outputs/my_first_training \
  --num-train-steps 1000 \
  --batch-size 32

# 훈련 진행 상황 확인
ls -la outputs/my_first_training/
```

---

## 6. 훈련된 모델 평가

### CLI로 평가

```bash
# 훈련된 모델 평가
python -m lerobot.scripts.lerobot_eval \
  --policy-path outputs/my_first_training/checkpoints/001000 \
  --env-name PushT-v0 \
  --num-eval-episodes 10 \
  --save-video true
```

### Python으로 평가

```python
from lerobot.policies.diffusion import DiffusionPolicy
from lerobot.envs.factory import make_env
import torch

# 로컬 체크포인트에서 정책 로드
policy = DiffusionPolicy.from_pretrained("outputs/my_first_training/checkpoints/001000")
policy.eval()

# 환경 생성
env = make_env("PushT-v0")

# 10개 에피소드 평가
success_count = 0
for episode_idx in range(10):
    observation, info = env.reset(seed=episode_idx)
    done = False
    step = 0

    while not done and step < 300:
        with torch.no_grad():
            action = policy.select_action(observation)
        observation, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        step += 1

    # 성공 여부 확인 (환경에 따라 다름)
    if info.get("is_success", False):
        success_count += 1

    print(f"Episode {episode_idx}: {step} steps, Success: {info.get('is_success', False)}")

print(f"\n성공률: {success_count}/10 = {success_count*10}%")
```

---

## 7. 사용 가능한 리소스 확인

### 데이터셋 목록

```python
from lerobot import available_datasets

print(f"총 {len(available_datasets)}개의 데이터셋 사용 가능")
print("\n처음 10개:")
for dataset_id in list(available_datasets)[:10]:
    print(f"  - {dataset_id}")
```

### 정책 목록

```python
from lerobot import available_policies

print("사용 가능한 정책:")
for policy_name in available_policies:
    print(f"  - {policy_name}")
```

### 환경 목록

```python
from lerobot import available_envs, available_tasks_per_env

print("사용 가능한 환경:")
for env_name in available_envs:
    tasks = available_tasks_per_env.get(env_name, [])
    print(f"  - {env_name}: {len(tasks)}개 작업")
    for task in tasks[:3]:  # 처음 3개만 출력
        print(f"      * {task}")
```

---

## 8. 실제 사용 예제 탐색

LeRobot은 풍부한 예제 코드를 제공합니다:

### 예제 디렉토리 구조

```
examples/
├── dataset/
│   ├── load_lerobot_dataset.py          # 데이터셋 로드
│   └── use_dataset_image_transforms.py  # 이미지 변환
├── training/
│   ├── train_policy.py                  # 정책 훈련
│   └── train_with_streaming.py          # 스트리밍 훈련
└── tutorial/
    ├── act/                             # ACT 튜토리얼
    ├── diffusion/                       # Diffusion 튜토리얼
    ├── smolvla/                         # SmolVLA 튜토리얼
    └── rl/                              # RL 튜토리얼
```

### 예제 실행

```bash
# 데이터셋 로드 예제
python examples/dataset/load_lerobot_dataset.py

# ACT 훈련 예제
python examples/tutorial/act/act_training_example.py

# Diffusion 사용 예제
python examples/tutorial/diffusion/diffusion_using_example.py
```

---

## 9. 커스텀 작업

### 커스텀 데이터셋 만들기

```python
from lerobot.datasets.utils import create_empty_dataset_info
import numpy as np

# 데이터셋 메타정보 생성
repo_id = "my_username/my_custom_dataset"
fps = 30
robot_type = "so100"

dataset_info = create_empty_dataset_info(repo_id, fps, robot_type)

# 에피소드 추가 (간단한 예제)
# 실제로는 lerobot_record 스크립트 사용 권장
```

### 커스텀 환경 사용

```python
import gymnasium as gym
from lerobot.envs.factory import make_env

# Gymnasium 환경을 LeRobot과 함께 사용
env = gym.make("CartPole-v1")

# 또는 커스텀 환경 등록
# (자세한 내용은 커스텀 환경 구현 가이드 참조)
```

---

## 다음 단계

이제 LeRobot의 기본을 익혔습니다! 다음 단계로 진행하세요:

### 초보자
1. [LeRobotDataset 이해하기](../02-core-concepts/10-LeRobotDataset-개요.md)
2. [정책 아키텍처 개요](../02-core-concepts/20-정책-아키텍처-개요.md)
3. [첫 번째 모델 훈련](../04-tutorials/60-첫-모델-훈련.md)

### 중급자
1. [커스텀 정책 구현](../07-advanced/111-커스텀-정책-구현.md)
2. [Multi-GPU 훈련](../07-advanced/110-MultiGPU-훈련.md)
3. [비동기 추론](../04-tutorials/67-비동기-추론.md)

### 고급 사용자
1. [실제 로봇으로 IL 수행](../04-tutorials/61-실제-로봇-IL.md)
2. [온라인 버퍼와 RL](../07-advanced/115-온라인-버퍼-RL.md)
3. [커스텀 로봇 통합](../06-hardware/95-커스텀-로봇-통합.md)

---

**참조 파일:**
- [examples/dataset/load_lerobot_dataset.py](../../../examples/dataset/load_lerobot_dataset.py)
- [examples/training/train_policy.py](../../../examples/training/train_policy.py)
- [README.md](../../../README.md) - Quick Start 섹션
