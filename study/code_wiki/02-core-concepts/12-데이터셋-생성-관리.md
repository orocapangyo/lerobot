# 데이터셋 생성과 관리

## 개요

이 문서에서는 자신만의 LeRobotDataset을 생성하고 관리하는 방법을 다룹니다.

---

## 데이터 수집 준비

### 필요한 구성 요소

1. **로봇 하드웨어** (또는 시뮬레이션)
2. **카메라** (선택사항, 비전 기반 정책의 경우)
3. **원격 조작 장치** (키보드, 게임패드, 리더 로봇 등)
4. **데이터 저장 공간** (에피소드당 100MB-1GB)

### 로봇 설정 확인

```bash
# 카메라 발견
python -m lerobot.scripts.lerobot_find_cameras

# 모터 포트 찾기
python -m lerobot.scripts.lerobot_find_port

# 로봇 보정
python -m lerobot.scripts.lerobot_calibrate \
  --robot-path lerobot/robots/configs/so100.yaml
```

---

## CLI로 데이터 수집

### 기본 녹화

```bash
python -m lerobot.scripts.lerobot_record \
  --robot-path lerobot/robots/configs/so100.yaml \
  --fps 30 \
  --repo-id my_username/my_dataset \
  --num-episodes 50 \
  --warmup-time-s 3 \
  --episode-time-s 30 \
  --reset-time-s 5
```

**주요 파라미터:**
- `--robot-path`: 로봇 설정 파일
- `--fps`: 초당 프레임 수 (보통 30)
- `--repo-id`: HuggingFace Hub 저장소 ID
- `--num-episodes`: 수집할 에피소드 수
- `--warmup-time-s`: 녹화 시작 전 대기 시간
- `--episode-time-s`: 최대 에피소드 길이
- `--reset-time-s`: 에피소드 간 리셋 시간

### 원격 조작과 함께

```bash
python -m lerobot.scripts.lerobot_record \
  --robot-path lerobot/robots/configs/so100.yaml \
  --teleop-path lerobot/teleoperators/configs/keyboard.yaml \
  --fps 30 \
  --repo-id my_username/my_dataset \
  --num-episodes 50
```

### 로컬에만 저장

```bash
python -m lerobot.scripts.lerobot_record \
  --robot-path lerobot/robots/configs/so100.yaml \
  --fps 30 \
  --local-files-only \
  --root ./my_local_dataset \
  --num-episodes 50
```

---

## Python으로 데이터 수집

### 기본 수집 스크립트

```python
from lerobot.datasets.utils import create_empty_dataset_info
from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.robots import make_robot
import torch
import numpy as np
from pathlib import Path

# 1. 데이터셋 메타정보 생성
repo_id = "my_username/my_custom_dataset"
root = Path("./data")
fps = 30
robot_type = "so100"

dataset_info = create_empty_dataset_info(
    repo_id=repo_id,
    fps=fps,
    robot_type=robot_type,
    keys=[
        "observation.images.cam_high",
        "observation.state",
        "action",
    ]
)

# 2. 로봇 연결
robot = make_robot("so100", robot_path="lerobot/robots/configs/so100.yaml")
robot.connect()

# 3. 데이터 수집
num_episodes = 10

for episode_idx in range(num_episodes):
    print(f"에피소드 {episode_idx} 시작...")

    # 에피소드 데이터 버퍼
    episode_data = {
        "observation.images.cam_high": [],
        "observation.state": [],
        "action": [],
        "timestamp": [],
    }

    # 로봇 초기화
    robot.reset()

    # 녹화 시작
    input("준비되면 Enter를 누르세요...")

    start_time = time.time()
    frame_idx = 0

    while time.time() - start_time < 30:  # 30초 녹화
        # 관측 수집
        obs = robot.get_observation()

        # 데이터 저장
        episode_data["observation.images.cam_high"].append(obs["images"]["cam_high"])
        episode_data["observation.state"].append(obs["state"])
        episode_data["action"].append(robot.get_action())  # 현재 액션
        episode_data["timestamp"].append(time.time() - start_time)

        # FPS 유지
        time.sleep(1.0 / fps)
        frame_idx += 1

    print(f"  {frame_idx} 프레임 수집 완료")

    # 4. 에피소드 저장
    # (실제로는 lerobot_record 스크립트 사용 권장)

robot.disconnect()
```

---

## 데이터셋 메타데이터 생성

### 기본 메타데이터

```python
from lerobot.datasets.utils import create_empty_dataset_info

# 메타정보 생성
dataset_info = create_empty_dataset_info(
    repo_id="my_username/my_dataset",
    fps=30,
    robot_type="so100",
    keys=[
        "observation.images.cam_high",
        "observation.state",
        "action",
    ]
)

# info.json으로 저장
import json
from pathlib import Path

meta_dir = Path("./data/meta")
meta_dir.mkdir(parents=True, exist_ok=True)

with open(meta_dir / "info.json", "w") as f:
    json.dump(dataset_info, f, indent=2)
```

### 추가 메타데이터

```python
# 작업 정보 추가
dataset_info["tasks"] = {
    "pick_red_cube": "빨간 큐브를 집는 작업",
    "place_in_box": "상자에 넣는 작업",
}

# 로봇 구성
dataset_info["robot_config"] = {
    "num_joints": 6,
    "gripper_type": "parallel",
    "workspace": [0.2, 0.5, -0.3, 0.3, 0.0, 0.4],  # x, y, z 범위
}

# 카메라 설정
dataset_info["camera_config"] = {
    "cam_high": {
        "width": 640,
        "height": 480,
        "fps": 30,
    }
}
```

---

## 통계 계산

데이터셋의 정규화 통계를 계산합니다.

### CLI로 통계 계산

```bash
python -m lerobot.datasets.compute_stats \
  --repo-id my_username/my_dataset \
  --local-files-only
```

### Python으로 통계 계산

```python
from lerobot.datasets.compute_stats import compute_stats
from lerobot.datasets import LeRobotDataset

# 데이터셋 로드
dataset = LeRobotDataset(
    "my_username/my_dataset",
    local_files_only=True
)

# 통계 계산
stats = compute_stats(dataset)

print("계산된 통계:")
print(f"  observation.state: mean={stats['observation.state']['mean']}")
print(f"  action: mean={stats['action']['mean']}")

# 통계 저장
import json
from pathlib import Path

stats_dir = Path("./data/stats")
stats_dir.mkdir(parents=True, exist_ok=True)

with open(stats_dir / "default.json", "w") as f:
    # numpy array를 list로 변환
    stats_serializable = {
        key: {
            "mean": value["mean"].tolist(),
            "std": value["std"].tolist(),
            "min": value["min"].tolist(),
            "max": value["max"].tolist(),
        }
        for key, value in stats.items()
    }
    json.dump(stats_serializable, f, indent=2)
```

### 통계 커스터마이징

```python
from lerobot.datasets.compute_stats import (
    compute_episode_stats,
    aggregate_stats
)

# 에피소드별 통계 계산
episode_stats = []
for ep_idx in range(dataset.num_episodes):
    start = dataset.episode_data_index["from"][ep_idx]
    end = dataset.episode_data_index["to"][ep_idx]

    ep_data = {}
    for i in range(start, end):
        frame = dataset[i]
        for key in ["observation.state", "action"]:
            if key not in ep_data:
                ep_data[key] = []
            ep_data[key].append(frame[key])

    # 에피소드 통계
    ep_stats = compute_episode_stats(ep_data)
    episode_stats.append(ep_stats)

# 전체 통계 집계
aggregated_stats = aggregate_stats(episode_stats)
```

---

## HuggingFace Hub 업로드

### Hub 로그인

```bash
# CLI로 로그인
huggingface-cli login

# 또는 토큰 직접 입력
export HF_TOKEN="your_token_here"
```

### 데이터셋 업로드

```python
from lerobot.datasets.push_dataset_to_hub import push_dataset_to_hub
from pathlib import Path

# 로컬 데이터셋 경로
local_dir = Path("./data")

# Hub에 업로드
push_dataset_to_hub(
    repo_id="my_username/my_dataset",
    local_dir=local_dir,
    private=False,  # 공개 데이터셋
    # private=True,  # 비공개 데이터셋
)

print("✓ 업로드 완료!")
print("https://huggingface.co/datasets/my_username/my_dataset")
```

### 점진적 업로드

```bash
# 에피소드를 수집하면서 점진적으로 업로드
python -m lerobot.scripts.lerobot_record \
  --robot-path lerobot/robots/configs/so100.yaml \
  --fps 30 \
  --repo-id my_username/my_dataset \
  --num-episodes 50 \
  --push-to-hub \
  --push-freq 10  # 10 에피소드마다 업로드
```

---

## 데이터셋 편집

### 에피소드 삭제

```bash
python -m lerobot.scripts.lerobot_edit_dataset \
  --repo-id my_username/my_dataset \
  --delete-episodes 5 10 15  # 에피소드 5, 10, 15 삭제
```

### Python으로 편집

```python
from lerobot.scripts.lerobot_edit_dataset import delete_episodes
from pathlib import Path

# 로컬 데이터셋
local_dir = Path("./data")

# 에피소드 삭제
delete_episodes(
    local_dir=local_dir,
    episode_indices=[5, 10, 15]
)

# 통계 재계산
from lerobot.datasets.compute_stats import compute_stats
from lerobot.datasets import LeRobotDataset

dataset = LeRobotDataset(str(local_dir), local_files_only=True)
stats = compute_stats(dataset)

# 저장
# ...
```

### 에피소드 병합

```python
# 두 데이터셋 병합
from lerobot.datasets.aggregate import aggregate_datasets

aggregate_datasets(
    output_repo_id="my_username/merged_dataset",
    input_repo_ids=[
        "my_username/dataset_part1",
        "my_username/dataset_part2",
    ]
)
```

---

## 데이터셋 검증

### 무결성 검사

```python
from lerobot.datasets import LeRobotDataset
import torch

def validate_dataset(repo_id):
    """데이터셋 검증"""
    print(f"데이터셋 검증: {repo_id}")

    # 로드
    try:
        dataset = LeRobotDataset(repo_id)
    except Exception as e:
        print(f"✗ 로드 실패: {e}")
        return False

    # 메타데이터 확인
    print(f"✓ 에피소드: {dataset.num_episodes}")
    print(f"✓ 프레임: {len(dataset)}")
    print(f"✓ FPS: {dataset.fps}")

    # 데이터 샘플링
    errors = []
    for i in range(min(100, len(dataset))):
        try:
            frame = dataset[i]

            # Shape 검증
            for key, value in frame.items():
                if isinstance(value, torch.Tensor):
                    if torch.isnan(value).any():
                        errors.append(f"프레임 {i}, 키 {key}: NaN 발견")
                    if torch.isinf(value).any():
                        errors.append(f"프레임 {i}, 키 {key}: Inf 발견")

        except Exception as e:
            errors.append(f"프레임 {i}: {str(e)}")

    if errors:
        print(f"✗ {len(errors)}개 오류:")
        for err in errors[:5]:
            print(f"  - {err}")
        return False

    print("✓ 검증 성공!")
    return True

# 검증 실행
validate_dataset("my_username/my_dataset")
```

---

## 데이터셋 버전 관리

### Git-style 버전 관리

HuggingFace Hub는 Git 기반이므로 버전 관리가 자동으로 됩니다.

```python
from huggingface_hub import HfApi

api = HfApi()

# 커밋 히스토리 확인
commits = api.list_repo_commits(
    repo_id="my_username/my_dataset",
    repo_type="dataset"
)

for commit in commits[:5]:
    print(f"{commit.commit_id[:7]}: {commit.title}")
```

### 특정 버전 로드

```python
# 특정 커밋에서 로드
dataset = LeRobotDataset(
    "my_username/my_dataset",
    revision="abc123def"  # 커밋 해시
)

# 또는 태그/브랜치
dataset = LeRobotDataset(
    "my_username/my_dataset",
    revision="v1.0"
)
```

### 버전 태깅

```bash
# Hub CLI로 태그 생성
huggingface-cli tag my_username/my_dataset \
  --tag v1.0 \
  --message "Initial release"
```

---

## 데이터 포맷 변환

### 다른 포맷에서 LeRobot으로

#### RLDS 포맷

```python
from lerobot.datasets.v30.convert_rlds_to_lerobot import convert_rlds_dataset

convert_rlds_dataset(
    rlds_dataset_path="./rlds_data",
    output_repo_id="my_username/converted_dataset",
    fps=30,
    robot_type="custom"
)
```

#### 커스텀 포맷

```python
from lerobot.datasets.utils import create_empty_dataset_info
import numpy as np
from pathlib import Path

# 1. 메타정보 생성
dataset_info = create_empty_dataset_info(
    repo_id="my_username/custom_converted",
    fps=30,
    robot_type="custom",
    keys=["observation.state", "action"]
)

# 2. 데이터 변환
output_dir = Path("./output")
output_dir.mkdir(exist_ok=True)

# 커스텀 데이터 로드
custom_data = load_your_custom_data()  # 사용자 함수

episode_idx = 0
for episode in custom_data:
    # 에피소드 데이터를 Parquet으로 저장
    episode_data = {
        "frame_index": np.arange(len(episode["observations"])),
        "timestamp": episode["timestamps"],
        "episode_index": [episode_idx] * len(episode["observations"]),
        "observation.state": episode["observations"],
        "action": episode["actions"],
    }

    # Parquet 저장
    import pandas as pd
    df = pd.DataFrame(episode_data)

    parquet_path = output_dir / f"data/chunk-000/episode_{episode_idx:06d}.parquet"
    parquet_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(parquet_path)

    episode_idx += 1

# 3. 통계 계산 및 저장
# ...
```

---

## 대용량 데이터셋 처리

### 청크 단위 처리

대용량 데이터셋은 청크로 나누어 처리:

```python
# 1000 에피소드씩 청크로 나누기
chunk_size = 1000
total_episodes = 10000

for chunk_idx in range(0, total_episodes, chunk_size):
    print(f"청크 {chunk_idx // chunk_size} 처리 중...")

    episodes = list(range(chunk_idx, min(chunk_idx + chunk_size, total_episodes)))

    # 데이터 수집
    collect_episodes(episodes)

    # 즉시 업로드
    push_to_hub(chunk_idx)
```

### 병렬 처리

```python
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

def process_episode(episode_idx):
    """에피소드 처리"""
    # 데이터 수집, 변환 등
    return processed_data

# 병렬로 에피소드 처리
num_workers = mp.cpu_count()

with ProcessPoolExecutor(max_workers=num_workers) as executor:
    results = list(executor.map(
        process_episode,
        range(num_episodes)
    ))
```

---

## 데이터셋 문서화

### README 작성

```markdown
# My Custom Robot Dataset

## 개요
이 데이터셋은 SO-100 로봇으로 수집한 pick-and-place 작업 데이터입니다.

## 통계
- 에피소드: 500개
- 평균 길이: 150 프레임
- FPS: 30
- 성공률: 85%

## 데이터 구성
- `observation.images.cam_high`: 상단 카메라 (640x480)
- `observation.state`: 6-DoF joint positions
- `action`: 6-DoF joint positions + gripper (7D)

## 수집 환경
- 로봇: SO-100
- 작업 공간: 30cm x 30cm
- 객체: 빨간/파란 큐브
- 조명: 자연광

## 사용 예제
\```python
from lerobot.datasets import LeRobotDataset

dataset = LeRobotDataset("my_username/my_dataset")
print(f"에피소드: {dataset.num_episodes}")
\```

## 라이선스
CC BY 4.0

## 인용
\```
@dataset{my_dataset_2024,
  author = {Your Name},
  title = {My Custom Robot Dataset},
  year = {2024},
  publisher = {HuggingFace},
  url = {https://huggingface.co/datasets/my_username/my_dataset}
}
\```
```

---

## 모범 사례

### 데이터 품질

1. **일관된 조명**: 데이터 수집 시 조명 조건 유지
2. **다양한 시작 위치**: 일반화를 위해 다양한 초기 상태
3. **성공 에피소드**: 실패보다 성공 에피소드 위주로 수집
4. **충분한 데이터**: 작업당 최소 50-100 에피소드

### 메타데이터

```python
# 각 에피소드에 메타데이터 추가
episode_metadata = {
    "success": True,
    "task": "pick_red_cube",
    "difficulty": "easy",
    "environment": "natural_light",
    "collector": "operator_1",
    "notes": "smooth execution",
}
```

### 버전 관리

- 데이터 추가 시 버전 업데이트
- 변경 사항 README에 기록
- 주요 변경 시 새 버전 태그

---

## 다음 단계

- [데이터 변환과 증강](13-데이터-변환-증강.md) - 데이터 전처리
- [첫 번째 모델 훈련](../04-tutorials/60-첫-모델-훈련.md) - 수집한 데이터로 훈련
- [실제 로봇으로 IL 수행](../04-tutorials/61-실제-로봇-IL.md) - 전체 워크플로우

---

**참조 파일:**
- [src/lerobot/scripts/lerobot_record.py](../../../src/lerobot/scripts/lerobot_record.py)
- [src/lerobot/datasets/compute_stats.py](../../../src/lerobot/datasets/compute_stats.py)
- [src/lerobot/datasets/push_dataset_to_hub/](../../../src/lerobot/datasets/push_dataset_to_hub/)
- [examples/port_datasets/](../../../examples/port_datasets/)
