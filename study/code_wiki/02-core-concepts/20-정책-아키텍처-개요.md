# 정책 아키텍처 개요

## 개요

LeRobot의 정책(Policy)은 관측(observation)을 받아 행동(action)을 출력하는 학습된 모델입니다. 모든 정책은 HuggingFace 스타일로 구현되어 있어 일관된 인터페이스를 제공합니다.

## 정책 분류

### Imitation Learning 정책

관측된 데모 데이터로부터 학습:

- **ACT (Action Chunking Transformer)**: CVAE + Transformer, 양팔 조작에 강점
- **Diffusion Policy**: Diffusion 모델 기반, 복잡한 행동 분포 모델링
- **VQ-BeT**: Vector Quantization + Behavior Transformer
- **SmolVLA**: 소형 Vision-Language-Action 모델
- **Pi0 / Pi0.5**: Flow-matching 기반 VLA 모델
- **GROOT**: Eagle2 비전 모델 + Action head

### Reinforcement Learning 정책

환경과의 상호작용을 통해 학습:

- **SAC (Soft Actor-Critic)**: Model-free, off-policy RL
- **TDMPC**: Model-based, 예측 모델과 MPC 결합

### 추론 최적화

- **RTC (Real-Time Chunking)**: Flow-matching 정책의 추론 속도 개선

---

## 정책 구조

모든 정책은 3개의 핵심 컴포넌트로 구성됩니다:

### 1. Configuration
설정 및 하이퍼파라미터 정의

```python
from dataclasses import dataclass
from lerobot.policies.pretrained import PreTrainedConfig

@dataclass
class ACTConfig(PreTrainedConfig):
    """ACT 정책 설정"""
    # 입출력 shape
    input_shapes: dict
    output_shapes: dict

    # 모델 구조
    dim_model: int = 512
    n_heads: int = 8
    dim_feedforward: int = 3200
    n_encoder_layers: int = 4
    n_decoder_layers: int = 1

    # 학습 설정
    lr: float = 1e-5
    lr_backbone: float = 1e-5
    weight_decay: float = 1e-4

    # ACT 특화 설정
    chunk_size: int = 100
    kl_weight: float = 10.0
    latent_dim: int = 32
```

### 2. Modeling
실제 신경망 구현

```python
import torch
import torch.nn as nn
from lerobot.policies.pretrained import PreTrainedPolicy

class ACTPolicy(PreTrainedPolicy):
    """ACT 정책 모델"""

    def __init__(self, config: ACTConfig):
        super().__init__(config)

        # 비전 백본
        self.backbone = ...

        # CVAE 인코더/디코더
        self.encoder = ...
        self.decoder = ...

        # Transformer
        self.transformer = ...

    def forward(self, batch: dict[str, torch.Tensor]) -> dict:
        """순전파"""
        # 관측 인코딩
        obs_features = self.encode_observation(batch)

        # CVAE로 latent action 샘플링
        latent = self.encode_action(batch) if self.training else None

        # Transformer로 action sequence 생성
        actions = self.decode_action(obs_features, latent)

        return {"actions": actions}

    def select_action(self, observation: dict) -> torch.Tensor:
        """추론 시 action 선택"""
        with torch.no_grad():
            batch = self.normalize(observation)
            output = self.forward(batch)
            action = self.unnormalize(output["actions"])
        return action
```

### 3. Processor
데이터 전처리 및 후처리

```python
from lerobot.processor.core import Processor

class ACTProcessor(Processor):
    """ACT 전처리/후처리"""

    def __init__(self, config):
        super().__init__(config)
        # 정규화 통계 로드
        self.load_stats()

    def preprocess(self, data: dict) -> dict:
        """입력 정규화"""
        # 이미지 정규화 (0-255 -> 0-1)
        data["observation.image"] = data["observation.image"] / 255.0

        # 상태 정규화 (평균 0, 표준편차 1)
        data["observation.state"] = (
            data["observation.state"] - self.state_mean
        ) / self.state_std

        return data

    def postprocess(self, output: dict) -> dict:
        """출력 역정규화"""
        # 액션 역정규화
        output["action"] = (
            output["action"] * self.action_std + self.action_mean
        )
        return output
```

---

## 공통 인터페이스

모든 정책은 `PreTrainedPolicy`를 상속하여 동일한 인터페이스를 제공합니다.

### 모델 로드

```python
from lerobot.policies.act import ACTPolicy

# HuggingFace Hub에서 로드
policy = ACTPolicy.from_pretrained("lerobot/act_aloha_insertion")

# 로컬 체크포인트에서 로드
policy = ACTPolicy.from_pretrained("path/to/checkpoint")
```

### 추론

```python
import torch

# 평가 모드 설정
policy.eval()

# GPU로 이동
policy = policy.to("cuda")

# 관측 데이터 준비
observation = {
    "observation.image": torch.randn(1, 3, 224, 224).cuda(),
    "observation.state": torch.randn(1, 14).cuda(),
}

# Action 생성
with torch.no_grad():
    action = policy.select_action(observation)

print(action.shape)  # (1, action_dim)
```

### 훈련

```python
# 훈련 모드 설정
policy.train()

# 옵티마이저 생성
optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-4)

# 배치 데이터
batch = {
    "observation.image": ...,
    "observation.state": ...,
    "action": ...,
}

# 순전파
output = policy(batch)

# 손실 계산
loss = policy.compute_loss(output, batch)

# 역전파
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

### 저장

```python
# 체크포인트 저장
policy.save_pretrained("path/to/save")

# HuggingFace Hub에 업로드
policy.push_to_hub("my_username/my_policy")
```

---

## 정책 팩토리

정책 이름으로 쉽게 생성:

```python
from lerobot.policies.factory import make_policy

# 정책 생성
policy = make_policy(
    policy_name="act",
    config_overrides={
        "input_shapes": {"observation.image": (3, 224, 224)},
        "output_shapes": {"action": (14,)},
        "chunk_size": 100,
    }
)
```

사용 가능한 정책:

```python
from lerobot import available_policies

print(available_policies)
# ['act', 'diffusion', 'vqbet', 'tdmpc', 'smolvla', 'pi0', 'pi05', 'groot', 'sac', 'rtc']
```

---

## 입출력 Shape

정책마다 입출력 형식이 다를 수 있습니다.

### 입력 (Observation)

```python
# 단일 카메라
observation = {
    "observation.image": torch.Tensor,      # (B, C, H, W) 또는 (B, T, C, H, W)
    "observation.state": torch.Tensor,      # (B, state_dim) 또는 (B, T, state_dim)
}

# 다중 카메라
observation = {
    "observation.images.cam_high": torch.Tensor,
    "observation.images.cam_wrist": torch.Tensor,
    "observation.state": torch.Tensor,
}

# VLA 모델 (언어 입력 포함)
observation = {
    "observation.image": torch.Tensor,
    "observation.state": torch.Tensor,
    "text": str,  # "pick up the red block"
}
```

### 출력 (Action)

```python
# 단일 action (기본)
output = {
    "action": torch.Tensor,  # (B, action_dim)
}

# Action chunking (ACT, Diffusion 등)
output = {
    "action": torch.Tensor,  # (B, chunk_size, action_dim)
}

# RL 정책 (가치 함수 포함)
output = {
    "action": torch.Tensor,
    "value": torch.Tensor,
    "log_prob": torch.Tensor,
}
```

---

## Configuration 시스템

### Draccus 기반 설정

LeRobot은 `draccus`를 사용한 dataclass 기반 설정을 사용합니다:

```python
from dataclasses import dataclass, field

@dataclass
class PolicyConfig:
    # 필수 파라미터
    input_shapes: dict
    output_shapes: dict

    # 선택적 파라미터 (기본값 포함)
    learning_rate: float = 1e-4
    batch_size: int = 32

    # 복잡한 기본값
    optimizer_kwargs: dict = field(default_factory=lambda: {"weight_decay": 1e-4})
```

### CLI 오버라이드

명령줄에서 설정 변경:

```bash
python train.py \
  --policy.learning_rate 5e-5 \
  --policy.batch_size 64 \
  --policy.optimizer_kwargs.weight_decay 1e-5
```

### 사전 정의 프리셋

```python
from lerobot.configs.policies import ACT_ALOHA_PRESET

# 프리셋 사용
config = ACT_ALOHA_PRESET.copy()
config["learning_rate"] = 5e-5  # 일부 수정

policy = ACTPolicy(config)
```

---

## 정책별 특징 비교

### ACT
- **장점**: 양팔 조작에 강함, action chunking으로 안정적
- **단점**: 큰 모델 크기, 긴 시퀀스 필요
- **적합**: ALOHA, 양팔 로봇

### Diffusion
- **장점**: 복잡한 행동 분포 표현, 안정적 학습
- **단점**: 느린 추론 속도 (반복적 디노이징)
- **적합**: PushT, 복잡한 조작 작업

### VQ-BeT
- **장점**: 이산 행동 공간, Transformer 활용
- **단점**: Codebook 설계 필요
- **적합**: 범용 조작 작업

### SmolVLA / Pi0 / GROOT
- **장점**: 언어 조건화, 범용성, 제로샷 일반화
- **단점**: 큰 모델 크기, 많은 데이터 필요
- **적합**: 다양한 작업, 언어 기반 제어

### SAC
- **장점**: 안정적 off-policy RL, 연속 행동
- **단점**: 많은 샘플 필요, 보상 함수 설계
- **적합**: 시뮬레이션 학습, RL이 필요한 작업

### TDMPC
- **장점**: 샘플 효율적, 예측 모델 활용
- **단점**: 모델 학습 오버헤드
- **적합**: 시뮬레이션, 모델 기반 RL

---

## 정책 선택 가이드

### 데이터가 많은 경우
- **Diffusion Policy**: 복잡한 분포 학습 가능
- **VLA 모델**: 범용성과 일반화

### 데이터가 적은 경우
- **ACT**: 작은 데이터셋에서도 작동
- **TDMPC**: 모델 기반으로 샘플 효율적

### 실시간 제어가 필요한 경우
- **ACT + RTC**: 빠른 추론
- **VQ-BeT**: 단일 forward pass

### 양팔 로봇
- **ACT**: ALOHA용으로 최적화됨

### 언어 제어가 필요한 경우
- **SmolVLA**, **Pi0**, **GROOT**: 언어 조건화 지원

### 강화학습이 필요한 경우
- **SAC**: 범용 off-policy
- **TDMPC**: 모델 기반
- **HIL-SERL**: 인간 개입 + RL

---

## 커스텀 정책 구현

기본 템플릿:

```python
from dataclasses import dataclass
import torch
import torch.nn as nn
from lerobot.policies.pretrained import PreTrainedPolicy, PreTrainedConfig

@dataclass
class MyPolicyConfig(PreTrainedConfig):
    """커스텀 정책 설정"""
    input_shapes: dict
    output_shapes: dict
    hidden_dim: int = 256

class MyPolicy(PreTrainedPolicy):
    """커스텀 정책"""

    def __init__(self, config: MyPolicyConfig):
        super().__init__(config)
        self.config = config

        # 신경망 구성
        self.encoder = nn.Sequential(...)
        self.decoder = nn.Sequential(...)

    def forward(self, batch: dict) -> dict:
        """순전파"""
        # 관측 인코딩
        features = self.encoder(batch["observation.state"])

        # 액션 생성
        action = self.decoder(features)

        return {"action": action}

    def compute_loss(self, output: dict, batch: dict) -> torch.Tensor:
        """손실 계산"""
        predicted_action = output["action"]
        target_action = batch["action"]

        # MSE 손실
        loss = nn.functional.mse_loss(predicted_action, target_action)

        return loss
```

---

## 다음 단계

정책별 상세 문서:
- [ACT 정책](21-ACT-정책.md)
- [Diffusion 정책](22-Diffusion-정책.md)
- [VQ-BeT 정책](23-VQBeT-정책.md)
- [비전-언어-행동 모델](25-VLA-정책.md)
- [SAC와 보상 모델](26-SAC-RL-정책.md)

---

**참조 파일:**
- [src/lerobot/policies/pretrained.py](../../../src/lerobot/policies/pretrained.py) - 베이스 클래스
- [src/lerobot/policies/factory.py](../../../src/lerobot/policies/factory.py) - 팩토리 패턴
- [src/lerobot/configs/policies.py](../../../src/lerobot/configs/policies.py) - 정책 설정
